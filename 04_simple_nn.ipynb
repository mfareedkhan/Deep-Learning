{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`Simple Neural Network` in Python using TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Importing the required libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "# import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Preprocessing:**\n",
    "(Steps before Creating a Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Load the Titanic Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Dropping Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Dropping rows with missing 'age' and 'embarked' values\n",
    "titanic.dropna(subset=['age', 'embarked'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Convert Categorical Data to Numerical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical variables to dummy variables\n",
    "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'who', 'deck'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Splitting the Data into Features (X) and Targets (y)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features and target\n",
    "X = titanic.drop(['survived', 'alive', 'embark_town', 'adult_male', 'alone'], axis=1) # axis = 1 means drop columns & axis = 0 means drop rows\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.5 Splitting the Data into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# 80% training and 20% test, random_state fixed the randomness of the split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.6 Standardizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this stage we have preprocessed the data and now we can move to the next step of creating a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Building the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Define the Layers of the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Simple Neural Network (Model)\n",
    "\n",
    "# define the layers of the model\n",
    "input_layer = tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)) # input layer, 10 neurons, relu activation function, input shape = number of features\n",
    "# > Dense layer is a fully connected layer\n",
    "\n",
    "# hidden_layer = tf.keras.layers.Dense(10, activation='relu') # hidden layer, 10 neurons, relu activation function\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid') # output layer, 1 neuron, sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is defining the architecture of a simple neural network using TensorFlow's Keras API. Here's a breakdown of what's happening:\n",
    "\n",
    "1. **Input Layer**: The first line of code is defining the input layer of the neural network. This layer will have 10 neurons (also known as nodes), and it uses the ReLU (Rectified Linear Unit) activation function. The `input_shape` parameter is set to the number of features in the training data (`X_train.shape[1]`), which tells the network how many inputs to expect.\n",
    "\n",
    "    ```python\n",
    "    input_layer = tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],))\n",
    "    ```\n",
    "\n",
    "- The `Dense` function is used to create a fully connected layer, meaning each neuron in this layer will be connected to all neurons in the next layer. \n",
    "- The `ReLU` activation function is commonly used in neural networks to introduce non-linearity into the model. It outputs the input directly if it's positive; otherwise, it outputs zero.\n",
    "\n",
    "2. **Output Layer**: The last line of code is defining the output layer of the neural network. This layer has just `1 neuron`, and it uses the `sigmoid` activation function.\n",
    "\n",
    "    ```python\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ```\n",
    "\n",
    "- The `sigmoid activation` function is commonly used for binary classification problems. It outputs a value between 0 and 1, which can be interpreted as the probability of the positive class in a binary classification problem.\n",
    "\n",
    "The hidden layer code is commented out in this snippet, but if it were included, it would add an additional layer of 10 neurons using the ReLU activation function between the input and output layers.\n",
    "\n",
    "> In summary, this code is setting up a simple neural network with one input layer and one output layer. The input layer takes in the features from the training data and passes them through the network. The output layer then makes a prediction based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Combine the Layers to Create the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the layers into a model\n",
    "model = tf.keras.models.Sequential([input_layer, \n",
    "                                    # hidden_layer, \n",
    "                                    output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is creating a `Sequential model` in TensorFlow's Keras API. A Sequential model is a linear stack of layers, where you can just add one layer after another.\n",
    "\n",
    "Here's a breakdown of what's happening:\n",
    "\n",
    "1. `tf.keras.models.Sequential`: This initializes a new Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially.\n",
    "\n",
    "2. `[input_layer, output_layer]`: This is a list of the layers that you want to add to the model. The order in which you add them is the order in which the data will flow through the network. In this case, the data will first go through the `input_layer`, and then it will go to the `output_layer`.\n",
    "\n",
    "The `hidden_layer` is commented out in this code, but if it were included, it would add an additional layer between the input and output layers.\n",
    "\n",
    "> In summary, this code is creating a new Sequential model and adding the input and output layers to it. The resulting `model` object represents your neural network, and it can be trained on your data using its `fit` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.3 Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is compiling the model. Compiling the model means `configuring the learning process before training the model`. It requires an optimizer, a loss function, and optionally, some metrics to monitor during training and testing.\n",
    "\n",
    "Here's a breakdown of what's happening:\n",
    "\n",
    "1. **optimizer='`adam`'**: The optimizer controls the learning rate. In this case, '`adam`' is used, which is an algorithm for `first-order gradient-based optimization of stochastic objective functions`. It's a popular choice because it works well in practice and requires little configuration.\n",
    "\n",
    "2. **loss='`binary_crossentropy`'**: This is the `loss function that the model will try to minimize`. For binary classification problems like this one, '`binary_crossentropy`' is a common choice. It's a measure of error for binary classification problems.\n",
    "\n",
    "3. **metrics=['accuracy']**`: This is a list of metrics to be evaluated by the model during training and testing. In this case, we're interested in the 'accuracy' of the model, which is the proportion of correct predictions.\n",
    "\n",
    "> In summary, this code is setting up the learning process by specifying the optimizer, loss function, and metrics for the model. After this step, the model will be ready to be trained with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 7ms/step - loss: 0.8636 - accuracy: 0.3849\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.7951 - accuracy: 0.4622\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.7410 - accuracy: 0.5185\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6945 - accuracy: 0.6766\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6564 - accuracy: 0.7083\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 0.7258\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.7487\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5694 - accuracy: 0.7540\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5463 - accuracy: 0.7680\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5258 - accuracy: 0.7856\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5085 - accuracy: 0.7961\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4943 - accuracy: 0.7996\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4836 - accuracy: 0.8049\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4739 - accuracy: 0.8049\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.8049\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8067\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4539 - accuracy: 0.8032\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.8032\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4444 - accuracy: 0.8049\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.8067\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4369 - accuracy: 0.8067\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4339 - accuracy: 0.8049\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4307 - accuracy: 0.8102\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4281 - accuracy: 0.8137\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4258 - accuracy: 0.8102\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8137\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4212 - accuracy: 0.8172\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4195 - accuracy: 0.8190\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4173 - accuracy: 0.8190\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4155 - accuracy: 0.8190\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.8190\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4123 - accuracy: 0.8207\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4108 - accuracy: 0.8225\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4091 - accuracy: 0.8190\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4078 - accuracy: 0.8225\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4066 - accuracy: 0.8225\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4052 - accuracy: 0.8225\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.4041 - accuracy: 0.8225\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 18ms/step - loss: 0.4032 - accuracy: 0.8278\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.4017 - accuracy: 0.8225\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 25ms/step - loss: 0.4006 - accuracy: 0.8243\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.3998 - accuracy: 0.8207\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.3989 - accuracy: 0.8207\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 14ms/step - loss: 0.3977 - accuracy: 0.8243\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 0.3971 - accuracy: 0.8243\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3962 - accuracy: 0.8243\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3947 - accuracy: 0.8260\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3940 - accuracy: 0.8278\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3934 - accuracy: 0.8313\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3925 - accuracy: 0.8278\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3916 - accuracy: 0.8295\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3910 - accuracy: 0.8295\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3904 - accuracy: 0.8330\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3897 - accuracy: 0.8348\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3891 - accuracy: 0.8330\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3882 - accuracy: 0.8330\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3878 - accuracy: 0.8348\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3869 - accuracy: 0.8330\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3863 - accuracy: 0.8330\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3856 - accuracy: 0.8348\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3851 - accuracy: 0.8366\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3846 - accuracy: 0.8401\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3840 - accuracy: 0.8383\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.3836 - accuracy: 0.8383\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3829 - accuracy: 0.8401\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3824 - accuracy: 0.8401\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3817 - accuracy: 0.8401\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.8401\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3809 - accuracy: 0.8418\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3799 - accuracy: 0.8418\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3797 - accuracy: 0.8366\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3789 - accuracy: 0.8348\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.3786 - accuracy: 0.8366\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.3781 - accuracy: 0.8383\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3775 - accuracy: 0.8383\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3771 - accuracy: 0.8366\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3763 - accuracy: 0.8383\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3761 - accuracy: 0.8366\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3755 - accuracy: 0.8366\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3753 - accuracy: 0.8366\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3747 - accuracy: 0.8383\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3744 - accuracy: 0.8383\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3738 - accuracy: 0.8401\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3737 - accuracy: 0.8366\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3729 - accuracy: 0.8366\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 16ms/step - loss: 0.3725 - accuracy: 0.8383\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3722 - accuracy: 0.8383\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3719 - accuracy: 0.8401\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3716 - accuracy: 0.8383\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3711 - accuracy: 0.8383\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3711 - accuracy: 0.8383\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3702 - accuracy: 0.8401\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3700 - accuracy: 0.8401\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3695 - accuracy: 0.8401\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3694 - accuracy: 0.8418\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3689 - accuracy: 0.8418\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3687 - accuracy: 0.8436\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3682 - accuracy: 0.8418\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3680 - accuracy: 0.8383\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3676 - accuracy: 0.8401\n",
      "CPU times: total: 28.2 s\n",
      "Wall time: 21.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21845e9bbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is training the model using the `fit` method. Here's a breakdown of what's happening:\n",
    "\n",
    "1. **model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)**: \n",
    "   - This line is training the model for 100 epochs on the training data (`X_train` and `y_train`). \n",
    "     - `epochs`: An epoch is one complete pass through the training data. The model will train on the entire training data for 100 epochs, which means it will see each sample in the training data 100 times.\n",
    "   - The `batch_size` parameter is set to 32, which means that the model will update its weights after every 32 samples. The `verbose` parameter is set to 1, which means that the method will output progress logs after each epoch.\n",
    "   - The `verbose=1` parameter controls the amount of information that's printed during training. Setting it to 1 means that the method will output progress logs after each epoch.\n",
    "\n",
    "**Observations from the output:**\n",
    "- The output shows the progress of the training process. \n",
    "- For each epoch, it shows the loss and accuracy of the model on the training data. \n",
    "- The loss is a measure of the model's error, and the accuracy is the proportion of correct predictions. \n",
    "- The goal of the training process is to minimize the loss and maximize the accuracy.\n",
    "\n",
    "> In summary, this code is training the model on the training data for 100 epochs, updating the model's weights after every 32 samples, and outputting progress logs after each epoch. The output shows that the model's accuracy on the training data is improving over time, which is a good sign that the model is learning from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 8ms/step - loss: 0.5159 - accuracy: 0.7483\n",
      "Test Accuracy: 0.748251736164093\n",
      "Test Loss: 0.5159400701522827\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is evaluating the performance of the trained model on the test data (`X_test` and `y_test`). Here's a breakdown of what's happening:\n",
    "\n",
    "1. `model.evaluate(X_test, y_test, verbose=1)`: This line is evaluating the model's performance on the test data. The `evaluate` method returns the loss value and metrics values for the model in test mode. In this case, the method returns the loss and accuracy because these were specified when compiling the model.\n",
    "\n",
    "2. `loss, accuracy = model.evaluate(X_test, y_test, verbose=1)`: This line is unpacking the returned values into the variables `loss` and `accuracy`.\n",
    "\n",
    "3. `print(f\"Test Accuracy: {accuracy}\")` and `print(f\"Test Loss: {loss}\")`: These lines are printing the test accuracy and test loss.\n",
    "\n",
    "**Observations from the output:**\n",
    "- The output shows the loss and accuracy of the model on the test data. \n",
    "- The loss is a measure of the model's error, and the accuracy is the proportion of correct predictions. \n",
    "- In this case, the model has an accuracy of about `74.8%` and a loss of about `0.516` on the test data.\n",
    "- Which means the model is able to predict the correct class about 74.8% of the time on the test data.\n",
    "\n",
    "> In summary, this code is evaluating the model's performance on the test data and printing the results. The output shows that the model's accuracy on the test data is slightly lower than its accuracy on the training data, which is a common occurrence and can indicate that the model is overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's see all the steps in action within one snippet of code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 4s 6ms/step - loss: 0.7363 - accuracy: 0.5940\n",
      "Epoch 2/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6832 - accuracy: 0.6450\n",
      "Epoch 3/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6417 - accuracy: 0.6837\n",
      "Epoch 4/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.6074 - accuracy: 0.7030\n",
      "Epoch 5/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5792 - accuracy: 0.7188\n",
      "Epoch 6/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5538 - accuracy: 0.7417\n",
      "Epoch 7/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5321 - accuracy: 0.7663\n",
      "Epoch 8/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.5133 - accuracy: 0.7680\n",
      "Epoch 9/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4969 - accuracy: 0.7715\n",
      "Epoch 10/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4828 - accuracy: 0.7786\n",
      "Epoch 11/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4702 - accuracy: 0.7786\n",
      "Epoch 12/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4593 - accuracy: 0.7873\n",
      "Epoch 13/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4503 - accuracy: 0.7926\n",
      "Epoch 14/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4427 - accuracy: 0.7996\n",
      "Epoch 15/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4354 - accuracy: 0.8014\n",
      "Epoch 16/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4296 - accuracy: 0.8049\n",
      "Epoch 17/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.4239 - accuracy: 0.8120\n",
      "Epoch 18/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.4196 - accuracy: 0.8137\n",
      "Epoch 19/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4157 - accuracy: 0.8155\n",
      "Epoch 20/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4116 - accuracy: 0.8190\n",
      "Epoch 21/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4089 - accuracy: 0.8172\n",
      "Epoch 22/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.4061 - accuracy: 0.8225\n",
      "Epoch 23/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.4035 - accuracy: 0.8243\n",
      "Epoch 24/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.4010 - accuracy: 0.8260\n",
      "Epoch 25/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3990 - accuracy: 0.8278\n",
      "Epoch 26/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8278\n",
      "Epoch 27/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3953 - accuracy: 0.8278\n",
      "Epoch 28/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3938 - accuracy: 0.8260\n",
      "Epoch 29/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3922 - accuracy: 0.8260\n",
      "Epoch 30/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3908 - accuracy: 0.8295\n",
      "Epoch 31/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3896 - accuracy: 0.8313\n",
      "Epoch 32/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3883 - accuracy: 0.8348\n",
      "Epoch 33/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3871 - accuracy: 0.8348\n",
      "Epoch 34/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3860 - accuracy: 0.8348\n",
      "Epoch 35/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3848 - accuracy: 0.8330\n",
      "Epoch 36/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.8330\n",
      "Epoch 37/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.8366\n",
      "Epoch 38/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3824 - accuracy: 0.8348\n",
      "Epoch 39/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3812 - accuracy: 0.8348\n",
      "Epoch 40/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3804 - accuracy: 0.8383\n",
      "Epoch 41/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3798 - accuracy: 0.8366\n",
      "Epoch 42/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3792 - accuracy: 0.8401\n",
      "Epoch 43/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3783 - accuracy: 0.8401\n",
      "Epoch 44/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3775 - accuracy: 0.8401\n",
      "Epoch 45/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3771 - accuracy: 0.8401\n",
      "Epoch 46/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3762 - accuracy: 0.8401\n",
      "Epoch 47/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3754 - accuracy: 0.8401\n",
      "Epoch 48/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3749 - accuracy: 0.8401\n",
      "Epoch 49/100\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3744 - accuracy: 0.8418\n",
      "Epoch 50/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3736 - accuracy: 0.8383\n",
      "Epoch 51/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3731 - accuracy: 0.8383\n",
      "Epoch 52/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3728 - accuracy: 0.8366\n",
      "Epoch 53/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8418\n",
      "Epoch 54/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8418\n",
      "Epoch 55/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3709 - accuracy: 0.8418\n",
      "Epoch 56/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3703 - accuracy: 0.8418\n",
      "Epoch 57/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.8418\n",
      "Epoch 58/100\n",
      "18/18 [==============================] - 0s 20ms/step - loss: 0.3695 - accuracy: 0.8418\n",
      "Epoch 59/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3692 - accuracy: 0.8418\n",
      "Epoch 60/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3686 - accuracy: 0.8418\n",
      "Epoch 61/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3682 - accuracy: 0.8418\n",
      "Epoch 62/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3676 - accuracy: 0.8436\n",
      "Epoch 63/100\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 0.3673 - accuracy: 0.8418\n",
      "Epoch 64/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3668 - accuracy: 0.8436\n",
      "Epoch 65/100\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3663 - accuracy: 0.8418\n",
      "Epoch 66/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3659 - accuracy: 0.8418\n",
      "Epoch 67/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3655 - accuracy: 0.8401\n",
      "Epoch 68/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3649 - accuracy: 0.8401\n",
      "Epoch 69/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3646 - accuracy: 0.8383\n",
      "Epoch 70/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3638 - accuracy: 0.8401\n",
      "Epoch 71/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3634 - accuracy: 0.8401\n",
      "Epoch 72/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3628 - accuracy: 0.8418\n",
      "Epoch 73/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3624 - accuracy: 0.8383\n",
      "Epoch 74/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3616 - accuracy: 0.8401\n",
      "Epoch 75/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3610 - accuracy: 0.8401\n",
      "Epoch 76/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3606 - accuracy: 0.8383\n",
      "Epoch 77/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.8418\n",
      "Epoch 78/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8383\n",
      "Epoch 79/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8401\n",
      "Epoch 80/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3587 - accuracy: 0.8401\n",
      "Epoch 81/100\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.3585 - accuracy: 0.8401\n",
      "Epoch 82/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3578 - accuracy: 0.8383\n",
      "Epoch 83/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.8418\n",
      "Epoch 84/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3569 - accuracy: 0.8401\n",
      "Epoch 85/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3562 - accuracy: 0.8418\n",
      "Epoch 86/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3560 - accuracy: 0.8453\n",
      "Epoch 87/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3556 - accuracy: 0.8453\n",
      "Epoch 88/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.8453\n",
      "Epoch 89/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3548 - accuracy: 0.8436\n",
      "Epoch 90/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3546 - accuracy: 0.8471\n",
      "Epoch 91/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3541 - accuracy: 0.8489\n",
      "Epoch 92/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3536 - accuracy: 0.8506\n",
      "Epoch 93/100\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 0.3531 - accuracy: 0.8471\n",
      "Epoch 94/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.8471\n",
      "Epoch 95/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.8471\n",
      "Epoch 96/100\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.3520 - accuracy: 0.8489\n",
      "Epoch 97/100\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.8506\n",
      "Epoch 98/100\n",
      "18/18 [==============================] - 0s 17ms/step - loss: 0.3517 - accuracy: 0.8489\n",
      "Epoch 99/100\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.3511 - accuracy: 0.8506\n",
      "Epoch 100/100\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.3505 - accuracy: 0.8506\n",
      "5/5 [==============================] - 1s 7ms/step - loss: 0.5085 - accuracy: 0.7692\n",
      "Test Accuracy: 0.7692307829856873\n",
      "Test Loss: 0.5085462927818298\n",
      "CPU times: total: 26 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#1 remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#2 Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "#3 Preprocessing\n",
    "#3.1 Dropping rows with missing 'age' and 'embarked' values\n",
    "titanic.dropna(subset=['age', 'embarked'], inplace=True)\n",
    "\n",
    "#3.2 Converting categorical variables to dummy variables\n",
    "titanic = pd.get_dummies(titanic, columns=['sex', 'embarked', 'class', 'who', 'deck'], drop_first=True)\n",
    "\n",
    "#4 Selecting features and target\n",
    "X = titanic.drop(['survived', 'alive', 'embark_town', 'adult_male', 'alone'], axis=1)\n",
    "y = titanic['survived']\n",
    "\n",
    "#5 Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#6 Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#7 Building the model\n",
    "input_layer = tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)) # input layer\n",
    "# hidden_layer = tf.keras.layers.Dense(10, activation='relu') # hidden layer\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "\n",
    "model = tf.keras.models.Sequential([input_layer, \n",
    "                                    # hidden_layer, \n",
    "                                    output_layer])\n",
    "\n",
    "#8 Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#9 Training the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "#10 Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment:** Plot the Training and Validation Accuracy and Loss for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check if `GPU` is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: Windows-10-10.0.19045-SP0\n",
      "Tensor Flow Version: 2.10.0\n",
      "\n",
      "Python 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:43:00) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas 2.0.3\n",
      "Scikit-Learn 1.3.0\n",
      "SciPy 1.10.1\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"SciPy {sp.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
